{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Introduction to tf.Variable\n",
        "## Creating Variables:\n",
        "Create a scalar variable.\n",
        "Create a vector variable.\n",
        "Create a matrix variable."
      ],
      "metadata": {
        "id": "J6uKv6WWZSLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "jiaKtgOVZn-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj4tsGQhcxmm"
      },
      "outputs": [],
      "source": [
        "scalar_example = tf.Variable(2, name = 'scalar')\n",
        "vector_example = tf.Variable([[1,2,3]], name = 'vector')\n",
        "matrix_example = tf.Variable([[2,3,4],[4,5,6],[7,8,9]], name = 'matrix')\n",
        "\n",
        "print(scalar_example)\n",
        "print(vector_example)\n",
        "print(matrix_example)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print values of the variables\n",
        "print(\"Scalar value:\", scalar_example.numpy())\n",
        "print(\"Vector values:\", vector_example.numpy())\n",
        "print(\"Matrix values:\", matrix_example.numpy())"
      ],
      "metadata": {
        "id": "8J9vyn-PaOmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scalar_example += 3.  # will not work\n",
        "print(scalar_example)"
      ],
      "metadata": {
        "id": "zlRNFLxYaZvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost = tf.constant(4)\n",
        "print(cost+4)"
      ],
      "metadata": {
        "id": "M-BxQrlHgiIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the variable by adding 3.0\n",
        "scalar_example.assign_add(3)\n",
        "print(scalar_example)\n",
        "print(scalar_example.numpy())"
      ],
      "metadata": {
        "id": "qbhEjZtZwgnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly assign a new value to scalar variable\n",
        "\n",
        "scalar_example.assign(10)\n",
        "print(scalar_example.numpy())\n"
      ],
      "metadata": {
        "id": "TldJS6oZbRSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly assign a new value to matrix variable\n",
        "print(matrix_example.numpy())\n",
        "\n",
        "matrix_example[0,1].assign(7)\n",
        "print(matrix_example.numpy())\n",
        "\n",
        "matrix_example[:,2].assign([0,40,60])\n",
        "print(matrix_example.numpy())"
      ],
      "metadata": {
        "id": "MW7NEfQFSpEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two variables\n",
        "var1 = tf.Variable(3.0, name='var1')\n",
        "var2 = tf.Variable(4.0, name='var2')\n",
        "\n",
        "# Perform a simple addition\n",
        "result = var1 + var2\n",
        "\n",
        "print(\"Result of addition:\", result.numpy())\n",
        "\n",
        "# Perform a multiplication\n",
        "result = var1 * var2\n",
        "\n",
        "print(\"Result of multiplication:\", result.numpy())"
      ],
      "metadata": {
        "id": "QiEqPsh2xMJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Gradient Tape\n",
        "In TensorFlow, the tf.GradientTape API records operations for automatic differentiation. It \"watches\" the operations executed within its context and builds a computation graph, which is then used to compute gradients of variables with respect to some loss or output."
      ],
      "metadata": {
        "id": "eVuituXHpoVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(w1, w2):\n",
        "  return 3*w1**2 + 2*w1*w2\n",
        "\n",
        "w1 = tf.Variable(5.0)\n",
        "w2 = tf.Variable(6.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  loss = f(w1,w2)\n",
        "\n",
        "gradient = tape.gradient(loss, [w1,w2])\n",
        "print(gradient)\n",
        "del tape"
      ],
      "metadata": {
        "id": "uNUy5aUAVr5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = tf.Variable(5.0)\n",
        "w2 = tf.Variable(6.0)\n",
        "\n",
        "with tf.GradientTape(persistent = True) as tape:    # use persistnt = True\n",
        "  loss = f(w1,w2)\n",
        "\n",
        "gradient_w1 = tape.gradient(loss, w1)\n",
        "gradient_w2 = tape.gradient(loss, w2)\n",
        "print(gradient_w1)\n",
        "print(gradient_w2)\n",
        "del tape\n"
      ],
      "metadata": {
        "id": "nw4ILY4FWP12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By setting persistent=True in the tf.GradientTape constructor, the tape can be used to compute gradients multiple times. After you are done, you should delete the tape using del tape to release resources."
      ],
      "metadata": {
        "id": "PqbPlEOwTxxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_variable = tf.Variable(initial_value=0.3)\n",
        "with tf.GradientTape() as tape:\n",
        "  result = tf.square(input_variable)\n",
        "  gradient = tape.gradient(result, input_variable)\n",
        "print(gradient)"
      ],
      "metadata": {
        "id": "LJ0rcKAB7FpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of GradientTape with constant input tensor. By default graident tape will only track operations involving variables.\n",
        "# if we try to compute gradients with regrad to anything else, it will generate None output. Hence use tape.watch\n",
        "\n",
        "const1 = tf.constant(4.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(const1)\n",
        "  result = tf.square(const1)\n",
        "gradient = tape.gradient(result, const1)\n",
        "print(gradient)\n",
        "\n"
      ],
      "metadata": {
        "id": "EQeyXphB7vel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use gradient tape with constant tensors\n",
        "\n",
        "const1 = tf.constant(5.)\n",
        "const2 = tf.constant(6.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(const1)\n",
        "  tape.watch(const2)\n",
        "  result = f(const1,const2)\n",
        "\n",
        "gradient = tape.gradient(result, [const1,const2])\n",
        "print(gradient)\n",
        "del tape"
      ],
      "metadata": {
        "id": "U5bd6XB_XgJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Another way\n",
        "\n",
        "const1 = tf.constant(5.)\n",
        "const2 = tf.constant(6.)\n",
        "\n",
        "with tf.GradientTape(persistent =True) as tape:\n",
        "  tape.watch(const1)\n",
        "  tape.watch(const2)\n",
        "  result = f(const1,const2)\n",
        "\n",
        "gradient_const1 = tape.gradient(result, const1)\n",
        "gradient_const2 = tape.gradient(result, const2)\n",
        "print(gradient_const1)\n",
        "print(gradient_const2)\n",
        "del tape"
      ],
      "metadata": {
        "id": "1guD0Cq7X7Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with Second-order gradients\n",
        "\n",
        "time = tf.Variable(3.)\n",
        "with tf.GradientTape() as outer:\n",
        "  with tf.GradientTape() as inner:\n",
        "    position = 5*time**2+7\n",
        "    velocity = inner.gradient(position, time)\n",
        "  acceleration = outer.gradient(velocity, time)\n",
        "print(velocity)\n",
        "print(acceleration)\n"
      ],
      "metadata": {
        "id": "OZk0drVd8sm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second-order gradients\n",
        "\n",
        "time = tf.Variable(3.)\n",
        "with tf.GradientTape(persistent = True) as outer:\n",
        "    position = 5*time**2+7\n",
        "    velocity = outer.gradient(position, time)\n",
        "acceleration = outer.gradient(velocity, time)\n",
        "print(velocity)\n",
        "print(acceleration)"
      ],
      "metadata": {
        "id": "EoDc7DdyWRNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Variables for Function\n",
        "Define variables and a function\n",
        "$$f(x,y) = (x-1)^2 + (y-1)^2 + xy$$\n",
        "Use TensorFlow to find the minimum of the function\n",
        "$f$ using gradient descent."
      ],
      "metadata": {
        "id": "Mg2orqzAp52A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(0.0, name = 'x')\n",
        "y = tf.Variable(0.0, name = 'y')\n",
        "\n",
        "def f_x(x,y):\n",
        "  f1 = (x-1)**2+ (y-1)**2 + x*y\n",
        "  return f1\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "epoch =  1000\n",
        "\n",
        "for iter in range(epoch):\n",
        "  with tf.GradientTape() as tape:\n",
        "     loss = f_x(x,y)\n",
        "  gradients = tape.gradient(loss, [x,y])\n",
        "  optimizer.apply_gradients(zip(gradients, [x,y]))\n",
        "\n",
        "print(f\"Optimized parameters are x : {x.numpy()} and y: {y.numpy()}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PBPzTvQCppau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(1.0)\n",
        "\n",
        "# Not working this way\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "  loss = f_x(x,y)\n",
        "  [df_dx, df_dy] = tape.gradient(loss,[x, y])\n",
        "  [d2f_dx2, d2f_dy2] = tape.gradient([df_dx, df_dy],[x, y])\n",
        "\n",
        "print(df_dx, df_dy)\n",
        "print(d2f_dx2, d2f_dy2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V3mWH3Et4Ng5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This way working\n",
        "\n",
        "with tf.GradientTape(persistent = True) as inne:\n",
        "    loss = f_x(x,y)\n",
        "    partial_derivative_x = inne.gradient(loss, x)\n",
        "    partial_derivative_y = inne.gradient(loss, y)\n",
        "    partial_derivative_xx = inne.gradient(partial_derivative_x, x)\n",
        "    partial_derivative_xy = inne.gradient(partial_derivative_y, x)\n",
        "    partial_derivative_yy = inne.gradient(partial_derivative_y, y)\n",
        "    partial_derivative_yx = inne.gradient(partial_derivative_x, y)\n",
        "print(partial_derivative_x)\n",
        "print(partial_derivative_y)\n",
        "print(partial_derivative_xx)\n",
        "print(partial_derivative_yy)\n",
        "print(partial_derivative_xy)\n",
        "print(partial_derivative_yx)"
      ],
      "metadata": {
        "id": "upem3rmA97gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative way working\n",
        "\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "  loss = f_x(x,y)\n",
        "  [df_dx, df_dy] = tape.gradient(loss,[x, y])\n",
        "  d2f_dx2 = tape.gradient(df_dx, x)\n",
        "  d2f_dy2 = tape.gradient(df_dy, y)\n",
        "  d2f_dydx = tape.gradient(df_dx, y)\n",
        "  d2f_dxdy = tape.gradient(df_dy, x)\n",
        "\n",
        "print(\"df/dx\", df_dx)\n",
        "print(\"df/dy\", df_dy)\n",
        "print(\"d2f/dx2\", d2f_dx2)\n",
        "print(\"d2f/dy2\", d2f_dy2)\n",
        "print(\"d2f/dydx\",d2f_dydx)\n",
        "print(\"d2f/dxdy\",d2f_dxdy)"
      ],
      "metadata": {
        "id": "46MytahqTcJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "## Consider the following complex function:\n",
        "$ f(x,y) = ae^{bx} \\sin(cx) + b\\cos(ax) + cx^{3}+ y^{2}$\n",
        "\n",
        "where:\n",
        "*   $a,b$  and $c$ are variables.\n",
        "*   $x,y$ are constants\n",
        "\n",
        "We will compute the gradients of $f$ with respect to $a, b,c$ at speciic value of $x, y$"
      ],
      "metadata": {
        "id": "BOCNdTNeBRMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "\n",
        "x = tf.constant(1.0)\n",
        "y = tf.constant(2.0)\n",
        "\n",
        "# Define Variables\n",
        "a = tf.Variable(1.0)\n",
        "b = tf.Variable(2.0)\n",
        "c = tf.Variable(3.0)\n",
        "\n",
        "# Define function\n",
        "def cmplx_fn(a,b,c,x,y):\n",
        "  #tape.watch([x,y])\n",
        "  L = a*tf.exp(b*x)*tf.sin(c*x)+b*tf.cos(a*x)+ c*x**3 +y**2\n",
        "  return L\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  loss = cmplx_fn(a,b,c,x,y)\n",
        "  [d_a,d_b,d_c,d_x,d_y] =tape.gradient(loss, [a,b,c,x,y])\n",
        "print(d_a, d_b, d_c, d_x, d_y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1EmZD9Gx-iTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In some situations we want to stop gradients from backpropagtaing through some part of the neural network\n",
        "# tf.stop_gradient function returns its input during forward pass ( acts like tf.identity()) but it does not\n",
        "#let grdaients through during backpropagation ( it acts like a constant)\n",
        "\n",
        "def math_func(x,y):\n",
        "  g = x*y\n",
        "  g = tf.stop_gradient(g)\n",
        "  return (x**2+y**2)*g\n",
        "\n",
        "var1_tensor = tf.Variable(3.0)\n",
        "var2_tensor = tf.Variable(4.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  loss = math_func(var1_tensor, var2_tensor)\n",
        "\n",
        "gardient = tape.gradient(loss, [var1_tensor, var2_tensor])\n",
        "print(gardient)\n"
      ],
      "metadata": {
        "id": "WgIewo6aEKkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rYniTtUjfVXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}